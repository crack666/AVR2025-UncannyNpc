# Unity NPC mit OpenAI Realtime API - Implementierungsplan

## ðŸŽ¯ Projektziel
Verbindung eines ReadyPlayerMe NPCs in Unity mit der OpenAI Realtime API fÃ¼r natÃ¼rliche Sprachinteraktionen in VR.

## ðŸ“‹ Aktuelle Situation
- âœ… Unity-Projekt mit ReadyPlayerMe Setup vorhanden
- âœ… OpenAI Realtime Console (WebRTC-Implementierung) als Referenz verfÃ¼gbar
- ðŸŽ¯ Ziel: C# Unity-Integration fÃ¼r Echtzeit-Audio-Chat mit LLM

## ðŸ—ï¸ Technische Architektur

### Option 1: WebSocket-basierte LÃ¶sung (Empfohlen)
**Vorteile fÃ¼r Unity:**
- Bessere C# WebSocket-Bibliotheken verfÃ¼gbar
- Einfachere Integration in Unity
- BewÃ¤hrte LÃ¶sung fÃ¼r Gamedev
- Weniger Browser-spezifische AbhÃ¤ngigkeiten

### Option 2: WebRTC-basierte LÃ¶sung
**Herausforderungen:**
- Begrenzte native C# WebRTC-UnterstÃ¼tzung
- Komplexere Audio-Pipeline
- Mehr externe Dependencies

## ðŸ“¦ BenÃ¶tigte Unity-Packages

### Core Dependencies
```
- Newtonsoft.Json (fÃ¼r JSON-Serialisierung)
- WebSocketSharp-netstandard oder Native WebSockets
- Unity Microphone API (built-in)
- Unity AudioSource (built-in)
```

### Audio Processing
```
- NAudio (optional, fÃ¼r erweiterte Audio-Verarbeitung)
- Unity WebRequest fÃ¼r HTTP-Calls
```

## ðŸ—‚ï¸ Projektstruktur (Unity Assets)

```
Assets/
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ OpenAI/
â”‚   â”‚   â”œâ”€â”€ RealtimeAPI/
â”‚   â”‚   â”‚   â”œâ”€â”€ RealtimeClient.cs          # Haupt-WebSocket-Client
â”‚   â”‚   â”‚   â”œâ”€â”€ RealtimeEventTypes.cs      # Event-Definitionen
â”‚   â”‚   â”‚   â”œâ”€â”€ RealtimeAudioManager.cs    # Audio Ein-/Ausgabe
â”‚   â”‚   â”‚   â””â”€â”€ RealtimeMessageHandler.cs  # Message Processing
â”‚   â”‚   â””â”€â”€ Models/
â”‚   â”‚       â”œâ”€â”€ SessionConfig.cs           # API-Konfiguration
â”‚   â”‚       â”œâ”€â”€ AudioChunk.cs              # Audio-Datenstrukturen
â”‚   â”‚       â””â”€â”€ ConversationState.cs       # GesprÃ¤chszustand
â”‚   â”œâ”€â”€ NPC/
â”‚   â”‚   â”œâ”€â”€ NPCController.cs               # NPC-Hauptsteuerung
â”‚   â”‚   â”œâ”€â”€ NPCAnimationController.cs      # Lippensync & Gesten
â”‚   â”‚   â”œâ”€â”€ NPCVoiceManager.cs             # Sprach-Integration
â”‚   â”‚   â””â”€â”€ NPCStateManager.cs             # GesprÃ¤chszustÃ¤nde
â”‚   â””â”€â”€ Managers/
â”‚       â”œâ”€â”€ GameManager.cs                 # Globale Spielsteuerung
â”‚       â”œâ”€â”€ UIManager.cs                   # UI-Steuerung
â”‚       â””â”€â”€ AudioManager.cs                # Audio-System
â”œâ”€â”€ Prefabs/
â”‚   â”œâ”€â”€ NPCPrefabs/                        # ReadyPlayerMe NPCs
â”‚   â””â”€â”€ UIPrefabs/                         # Interface-Elemente
â””â”€â”€ Settings/
    â””â”€â”€ OpenAISettings.cs                  # API-Konfiguration
```

## ðŸ”§ Implementierungsschritte

### Phase 1: Grundlegendes Setup (Week 1)
1. **Unity WebSocket Integration**
   - WebSocketSharp-netstandard Package installieren
   - Basis WebSocket-Connection zu OpenAI testen
   - API-Key Management implementieren

2. **Audio Pipeline Setup**
   - Unity Microphone Input konfigurieren
   - Audio-Encoding (PCM16 -> Base64) implementieren
   - AudioSource fÃ¼r Playback einrichten

3. **Event System Design**
   - Event-basierte Architektur fÃ¼r API-Messages
   - UnityEvent-Integration fÃ¼r UI-Updates

### Phase 2: Core API Integration (Week 2)
1. **RealtimeClient.cs Implementierung**
   ```csharp
   public class RealtimeClient : MonoBehaviour 
   {
       private WebSocket ws;
       private string apiKey;
       private bool isConnected;
       
       public UnityEvent<string> OnMessageReceived;
       public UnityEvent<AudioClip> OnAudioReceived;
       
       public async Task ConnectAsync()
       public void SendAudioChunk(byte[] audioData)
       public void SendTextMessage(string message)
   }
   ```

2. **Audio Processing**
   - Microphone-Input in Real-time verarbeiten
   - Audio-Chunks in Base64 konvertieren
   - Empfangene Audio-Daten zu AudioClip konvertieren

3. **Message Handling**
   - JSON-Event-Parsing
   - Async/Await-Pattern fÃ¼r Unity
   - Error-Handling & Reconnection-Logic

### Phase 3: NPC Integration (Week 3)
1. **NPCController.cs**
   ```csharp
   public class NPCController : MonoBehaviour
   {
       [SerializeField] private RealtimeClient realtimeClient;
       [SerializeField] private Animator npcAnimator;
       [SerializeField] private AudioSource audioSource;
       
       private void OnEnable()
       {
           realtimeClient.OnAudioReceived.AddListener(PlayNPCResponse);
           realtimeClient.OnMessageReceived.AddListener(HandleNPCMessage);
       }
   }
   ```

2. **Animation Integration**
   - Lippensync mit empfangenen Audio-Daten
   - Idle-Animationen wÃ¤hrend GesprÃ¤chen
   - Gesten-System basierend auf Kontext

3. **Voice Activity Detection**
   - Microphone-Level-Monitoring
   - Push-to-Talk vs. kontinuierliche Erkennung
   - Unity-Input-System Integration

### Phase 4: Advanced Features (Week 4)
1. **Conversation Management**
   - Multi-Turn-Dialoge verwalten
   - Kontext zwischen GesprÃ¤chen speichern
   - NPC-PersÃ¶nlichkeits-System

2. **UI Integration**
   - Conversation-Log-Display
   - Audio-Level-Visualization
   - Connection-Status-Indicators

3. **Performance Optimization**
   - Audio-Buffer-Management
   - Memory-Pool fÃ¼r Audio-Chunks
   - Threading fÃ¼r Audio-Processing

## ðŸŽšï¸ Audio-Pipeline Details

### Input-Processing
```csharp
// Microphone -> PCM16 -> Base64 -> WebSocket
private IEnumerator ProcessMicrophoneInput()
{
    while (isRecording)
    {
        float[] samples = new float[sampleRate / 10]; // 100ms chunks
        microphone.GetData(samples, 0);
        
        byte[] pcmData = ConvertToPCM16(samples);
        string base64Audio = Convert.ToBase64String(pcmData);
        
        SendAudioEvent(base64Audio);
        yield return new WaitForSeconds(0.1f);
    }
}
```

### Output-Processing
```csharp
// WebSocket -> Base64 -> PCM16 -> AudioClip -> AudioSource
private void ProcessIncomingAudio(string base64Audio)
{
    byte[] audioData = Convert.FromBase64String(base64Audio);
    AudioClip clip = CreateAudioClipFromPCM(audioData);
    
    npcAudioSource.clip = clip;
    npcAudioSource.Play();
    
    // Trigger lip sync animation
    StartLipSync(clip.length);
}
```

## ðŸ”’ Sicherheit & Konfiguration

### API-Key Management
```csharp
[CreateAssetMenu(fileName = "OpenAISettings", menuName = "OpenAI/Settings")]
public class OpenAISettings : ScriptableObject
{
    [SerializeField] private string apiKey;
    [SerializeField] private string baseUrl = "wss://api.openai.com/v1/realtime";
    [SerializeField] private string model = "gpt-4o-realtime-preview-2024-12-17";
    
    public string GetApiKey() => apiKey;
}
```

## ðŸ§ª Testing & Debugging

### Debug-Features
- WebSocket-Message-Logger
- Audio-Waveform-Visualizer
- Latenz-Monitoring
- Error-Reporting-System

### Performance-Metriken
- Audio-Latenz (Input -> Output)
- WebSocket-Response-Times
- Memory-Usage-Tracking
- Frame-Rate-Impact-Analysis

## ðŸŽ¯ Erwartete Herausforderungen

### Technische Herausforderungen
1. **Audio-Latenz minimieren** - Optimierung der gesamten Pipeline
2. **WebSocket-StabilitÃ¤t** - Reconnection-Logic implementieren
3. **Unity-Threading** - Async-Operations in Unity-Context
4. **Memory-Management** - Audio-Buffers effizient verwalten

### LÃ¶sungsansÃ¤tze
1. **Audio-Chunks klein halten** (50-100ms)
2. **Connection-Pooling** fÃ¼r bessere StabilitÃ¤t
3. **Unity-Jobs-System** fÃ¼r Audio-Processing
4. **Object-Pooling** fÃ¼r Audio-Clips

## ðŸ“ˆ Erfolgskriterien

### MVP (Minimum Viable Product)
- âœ… Stabile WebSocket-Verbindung zu OpenAI
- âœ… Audio-Input vom Mikrofon
- âœ… Audio-Output Ã¼ber NPC
- âœ… Basis-Lippensync

### Full Feature Set
- âœ… NatÃ¼rliche GesprÃ¤chsfÃ¼hrung
- âœ… Emotion-basierte Animationen
- âœ… Voice Activity Detection
- âœ… Multi-NPC-Support
- âœ… Conversation-History
- âœ… Performance-optimiert fÃ¼r VR

## ðŸ”„ NÃ¤chste Schritte

1. **Sofort:** WebSocketSharp Package installieren
2. **Tag 1-2:** Basis WebSocket-Connection implementieren
3. **Tag 3-5:** Audio-Pipeline aufbauen
4. **Tag 6-7:** NPC-Integration testen
5. **Week 2:** Refinement und Performance-Optimierung

---

## ðŸ“š Referenzen

### OpenAI Realtime API
- **WebSocket Endpoint:** `wss://api.openai.com/v1/realtime`
- **Model:** `gpt-4o-realtime-preview-2024-12-17`
- **Audio Format:** PCM16, 24kHz, mono
- **Event-basierte Kommunikation**

### Unity-spezifische Ãœberlegungen
- **Microphone.Start()** fÃ¼r Audio-Input
- **AudioClip.Create()** fÃ¼r dynamische Audio-Clips
- **Coroutines** fÃ¼r asynchrone Audio-Verarbeitung
- **UnityEvents** fÃ¼r lose gekoppelte Kommunikation

### Performance-Optimierungen
- **Audio-Buffer-GrÃ¶ÃŸe:** 100ms chunks
- **Thread-Safety:** Main-Thread fÃ¼r Unity-API-Calls
- **Memory-Management:** Object-Pooling fÃ¼r hÃ¤ufige Allocations
- **Error-Handling:** Graceful degradation bei Verbindungsproblemen
